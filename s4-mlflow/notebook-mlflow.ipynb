{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23b9432",
   "metadata": {},
   "source": [
    "# MLflow Model Training and Serving Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end machine learning pipeline using MLflow with the DigitalHub SDK. We'll work with the Iris dataset, train a classifier with hyperparameter tuning, track experiments with MLflow, and deploy the model as a REST API service.\n",
    "\n",
    "## Overview\n",
    "- **Model Training**: Train an SVM classifier with grid search hyperparameter tuning\n",
    "- **MLflow Integration**: Automatic logging of datasets, parameters, and metrics\n",
    "- **Model Serving**: Deploy the trained MLflow model as a REST API endpoint\n",
    "- **Orchestrate**: Create a workflow pipeline to automate the ML process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc6198",
   "metadata": {},
   "source": [
    "## Setup and Function Definitions\n",
    "\n",
    "First, we'll create the necessary directory structure and define all the functions we'll need for our MLflow pipeline. All functions will be stored in a single `src/functions.py` file for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0616c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398014f2",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our main functions file with the following components:\n",
    "\n",
    "- **`train_model`**: Trains an SVM classifier with grid search hyperparameter tuning using MLflow autologging\n",
    "- **MLflow Integration**: Automatically logs datasets, parameters, metrics, and model artifacts\n",
    "- **Model Registration**: Registers the trained model in DigitalHub with MLflow metadata\n",
    "\n",
    "The function uses MLflow's autolog feature to automatically capture training metrics, parameters, and artifacts, then integrates them with DigitalHub's model management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/functions.py\"\n",
    "import mlflow\n",
    "from digitalhub import from_mlflow_run, get_mlflow_model_metrics\n",
    "from digitalhub_runtime_python import handler\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "@handler(outputs=[\"model\"])\n",
    "def train_model(project):\n",
    "    \"\"\"\n",
    "    Train an SVM classifier on the Iris dataset with hyperparameter tuning using MLflow\n",
    "    \"\"\"\n",
    "    # Enable MLflow autologging for sklearn\n",
    "    mlflow.sklearn.autolog(log_datasets=True)\n",
    "\n",
    "    # Load Iris dataset\n",
    "    iris = datasets.load_iris()\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n",
    "    svc = svm.SVC()\n",
    "    clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "    # Train model with grid search\n",
    "    clf.fit(iris.data, iris.target)\n",
    "\n",
    "    # Get MLflow run information\n",
    "    run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "    # Extract MLflow run artifacts and metadata for DigitalHub integration\n",
    "    model_params = from_mlflow_run(run_id)\n",
    "    metrics = get_mlflow_model_metrics(run_id)\n",
    "\n",
    "    # Register model in DigitalHub with MLflow metadata\n",
    "    model = project.log_model(name=\"iris-classifier\", kind=\"mlflow\", **model_params)\n",
    "    model.log_metrics(metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8dc4a",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Now we'll initialize our DigitalHub project using consistent naming with other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50647a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "p_name = \"tutorial-project\"\n",
    "project = dh.get_or_create_project(p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c901d",
   "metadata": {},
   "source": [
    "## Step 1: Model Training with MLflow\n",
    "\n",
    "We'll create and run our MLflow-integrated training function. This will train an SVM classifier on the Iris dataset with hyperparameter tuning, automatically logging all experiments with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = project.new_function(\n",
    "    name=\"train-mlflow-model\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"train_model\",\n",
    "    requirements=[\"numpy<2\", \"mlflow<3\", \"scikit-learn <= 1.6.1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbe58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_run = train_fn.run(action=\"job\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c8e63",
   "metadata": {},
   "source": [
    "## Step 2: Model Serving\n",
    "\n",
    "Now we'll deploy our trained MLflow model as a REST API service. This will allow us to make predictions via HTTP requests using the MLflow serving infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model_run.output(\"model\")\n",
    "serve_func = project.new_function(\n",
    "    name=\"serve-mlflow-model\",\n",
    "    kind=\"mlflowserve\",\n",
    "    model_name=model.name,\n",
    "    path=model.key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\"serve\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef89bbb",
   "metadata": {},
   "source": [
    "### Test the Model API\n",
    "\n",
    "Let's test our deployed MLflow model by making a prediction request with sample Iris data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a83f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load some test data from the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data[0:2].tolist()\n",
    "json_payload = {\n",
    "    \"inputs\": [{\"name\": \"input-0\", \"shape\": [-1, 4], \"datatype\": \"FP64\", \"data\": data}]\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "result = serve_run.invoke(model_name=model.name, json=json_payload).json()\n",
    "print(\"Prediction result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205b89b",
   "metadata": {},
   "source": [
    "## Pipeline Orchestration\n",
    "\n",
    "Now let's create a workflow that orchestrates the MLflow training process. This pipeline uses Hera (Argo Workflows) to define the execution flow for our MLflow-based ML pipeline.\n",
    "\n",
    "The pipeline includes:\n",
    "1. **A**: Train model with MLflow integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/pipeline.py\"\n",
    "from hera.workflows import Workflow, Steps\n",
    "from digitalhub_runtime_hera.dsl import step\n",
    "\n",
    "\n",
    "def pipeline():\n",
    "    with Workflow(entrypoint=\"dag\") as w:\n",
    "        with Steps(name=\"dag\"):\n",
    "            A = step(template={\"action\":\"job\"}, function=\"train-mlflow-model\", outputs=[\"model\"])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cce471",
   "metadata": {},
   "source": [
    "### Execute the Complete Pipeline\n",
    "\n",
    "Finally, let's create and execute our complete MLflow pipeline workflow. This will run the training process in an automated, orchestrated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.new_workflow(\n",
    "    name=\"mlflow-pipeline\",\n",
    "    kind=\"hera\",\n",
    "    code_src=\"src/pipeline.py\",\n",
    "    handler=\"pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c39352",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run(\"build\", wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_run = workflow.run(\"pipeline\", wait=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
