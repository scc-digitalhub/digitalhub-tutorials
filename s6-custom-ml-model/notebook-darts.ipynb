{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a91309",
   "metadata": {},
   "source": [
    "# Custom Time Series Model Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to build, train, and serve a custom time series forecasting model using Darts (a Python library for time series) with the DigitalHub SDK. We'll work with the Air Passengers dataset, train a NBEATS model, and deploy it as a REST API service.\n",
    "\n",
    "## Overview\n",
    "- **Data Processing**: Load and prepare the Air Passengers time series dataset\n",
    "- **Model Training**: Train a NBEATS deep learning model for time series forecasting\n",
    "- **Model Packaging**: Package the trained model for deployment\n",
    "- **Model Serving**: Deploy the model as a REST API with custom serving logic\n",
    "- **Orchestrate**: Create a workflow pipeline to automate the entire process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0ecc6",
   "metadata": {},
   "source": [
    "## Setup and Function Definitions\n",
    "\n",
    "First, we'll create the necessary directory structure and define all the functions we'll need for our time series pipeline. All functions will be stored in a single `src/functions.py` file for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0006d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e9436",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our main functions file with the following components:\n",
    "\n",
    "- **`train_model`**: Trains a NBEATS model on the Air Passengers dataset with evaluation metrics\n",
    "- **`init_context`**: Initializes the serving context by loading the trained model\n",
    "- **`serve_predictions`**: Serves time series predictions via REST API\n",
    "\n",
    "The functions use Darts library for time series modeling and implement custom serving logic for real-time forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/functions.py\"\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.metrics import mae, mape, smape\n",
    "from darts.models import NBEATSModel\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "@handler(outputs=[\"model\"])\n",
    "def train_model(project):\n",
    "    \"\"\"\n",
    "    Train a NBEATS model on the Air Passengers dataset\n",
    "    \"\"\"\n",
    "    # Load Air Passengers dataset\n",
    "    series = AirPassengersDataset().load()\n",
    "    train, test = series[:-36], series[-36:]\n",
    "\n",
    "    # Configure and train NBEATS model\n",
    "    model = NBEATSModel(input_chunk_length=24, output_chunk_length=12, n_epochs=200, random_state=0)\n",
    "    model.fit(train)\n",
    "\n",
    "    # Make predictions for evaluation\n",
    "    pred = model.predict(n=36)\n",
    "\n",
    "    # Save model artifacts\n",
    "    model.save(\"predictor_model.pt\")\n",
    "    with ZipFile(\"predictor_model.pt.zip\", \"w\") as z:\n",
    "        z.write(\"predictor_model.pt\")\n",
    "        z.write(\"predictor_model.pt.ckpt\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\"mape\": mape(test, pred), \"smape\": smape(test, pred), \"mae\": mae(test, pred)}\n",
    "\n",
    "    # Register model in DigitalHub\n",
    "    model_artifact = project.log_model(\n",
    "        name=\"air-passengers-forecaster\",\n",
    "        kind=\"model\",\n",
    "        source=\"predictor_model.pt.zip\",\n",
    "        algorithm=\"darts.models.NBEATSModel\",\n",
    "        framework=\"darts\",\n",
    "    )\n",
    "    model_artifact.log_metrics(metrics)\n",
    "    return model_artifact\n",
    "\n",
    "\n",
    "def init_context(context, model_key):\n",
    "    \"\"\"\n",
    "    Initialize serving context by loading the trained model\n",
    "    \"\"\"\n",
    "    model = context.project.get_model(model_key)\n",
    "    path = model.download()\n",
    "    local_path_model = \"extracted_model/\"\n",
    "\n",
    "    # Extract model from zip file\n",
    "    with ZipFile(path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(local_path_model)\n",
    "\n",
    "    # Load the NBEATS model\n",
    "    input_chunk_length = 24\n",
    "    output_chunk_length = 12\n",
    "    name_model_local = local_path_model + \"predictor_model.pt\"\n",
    "    mm = NBEATSModel(input_chunk_length, output_chunk_length).load(name_model_local)\n",
    "\n",
    "    setattr(context, \"model\", mm)\n",
    "\n",
    "\n",
    "def serve_predictions(context, event):\n",
    "    \"\"\"\n",
    "    Serve time series predictions via REST API\n",
    "    \"\"\"\n",
    "    if isinstance(event.body, bytes):\n",
    "        body = json.loads(event.body)\n",
    "    else:\n",
    "        body = event.body\n",
    "\n",
    "    context.logger.info(f\"Received event: {body}\")\n",
    "    inference_input = body[\"inference_input\"]\n",
    "\n",
    "    # Convert input to Darts TimeSeries format\n",
    "    pdf = pd.DataFrame(inference_input)\n",
    "    pdf[\"date\"] = pd.to_datetime(pdf[\"date\"], unit=\"ms\")\n",
    "\n",
    "    ts = TimeSeries.from_dataframe(pdf, time_col=\"date\", value_cols=\"value\")\n",
    "\n",
    "    # Make predictions\n",
    "    output_chunk_length = 12\n",
    "    result = context.model.predict(n=output_chunk_length * 2, series=ts)\n",
    "\n",
    "    # Convert result to JSON format\n",
    "    jsonstr = result.pd_dataframe().reset_index().to_json(orient=\"records\")\n",
    "    return json.loads(jsonstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ffa15",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Now we'll initialize our DigitalHub project using consistent naming with other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c34cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "p_name = \"tutorial-project\"\n",
    "project = dh.get_or_create_project(p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660f003",
   "metadata": {},
   "source": [
    "## Step 1: Model Training\n",
    "\n",
    "We'll create and run our NBEATS training function. This will train a deep learning model for time series forecasting on the Air Passengers dataset, which contains monthly passenger numbers from 1949 to 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = project.new_function(\n",
    "    name=\"train-time-series-model\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"train_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17680dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_build = train_fn.run(\n",
    "    \"build\",\n",
    "    instructions=[\n",
    "        \"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\n",
    "        \"pip3 install darts==0.30.0 patsy scikit-learn\",\n",
    "    ],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = train_fn.run(\"job\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44140a",
   "metadata": {},
   "source": [
    "Let's examine the trained model and its performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_run.output(\"model\")\n",
    "print(\"Time series model metrics:\")\n",
    "print(model.spec.get(\"metrics\", {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f641d",
   "metadata": {},
   "source": [
    "## Step 2: Model Serving\n",
    "\n",
    "Now we'll deploy our trained time series model as a REST API service. This involves creating a serving function with custom initialization and prediction logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69799fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_func = project.new_function(\n",
    "    name=\"serve-time-series-model\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"serve_predictions\",\n",
    "    init_function=\"init_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_build_model_serve = serve_func.run(\n",
    "    \"build\",\n",
    "    instructions=[\n",
    "        \"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\n",
    "        \"pip3 install darts==0.30.0 patsy scikit-learn\",\n",
    "    ],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b911f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\"serve\", init_parameters={\"model_key\": model.key}, labels=[\"time-series-service\"], wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d6a30",
   "metadata": {},
   "source": [
    "### Test the Time Series API\n",
    "\n",
    "Let's test our deployed model by sending time series data for forecasting. We'll use the last 24 data points from the Air Passengers dataset as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install darts locally for testing (if not already installed)\n",
    "%pip install darts==0.30.0 --quiet\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from darts.datasets import AirPassengersDataset\n",
    "\n",
    "# Load test data\n",
    "series = AirPassengersDataset().load()\n",
    "val = series[-24:]  # Last 24 points for prediction\n",
    "json_value = json.loads(val.to_json())\n",
    "\n",
    "# Prepare input data in the expected format\n",
    "data = map(\n",
    "    lambda x, y: {\"value\": x[0], \"date\": datetime.timestamp(datetime.strptime(y, \"%Y-%m-%dT%H:%M:%S.%f\")) * 1000},\n",
    "    json_value[\"data\"],\n",
    "    json_value[\"index\"],\n",
    ")\n",
    "inputs = {\"inference_input\": list(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602553c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction request\n",
    "result = serve_run.invoke(json=inputs).json()\n",
    "print(\"Time series forecast result:\")\n",
    "print(f\"Predicted {len(result)} future time points\")\n",
    "print(\"Sample predictions:\", result[:3])  # Show first 3 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b04f5",
   "metadata": {},
   "source": [
    "## Pipeline Orchestration\n",
    "\n",
    "Now let's create a workflow that orchestrates the entire time series modeling process. This pipeline uses Hera (Argo Workflows) to define the execution flow:\n",
    "\n",
    "1. **A**: Build training environment \n",
    "2. **B**: Build serving environment \n",
    "3. **C**: Train the time series model\n",
    "4. **D**: Deploy model service\n",
    "\n",
    "The pipeline handles the complex dependencies and environment setup required for the Darts time series library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/pipeline.py\"\n",
    "from digitalhub_runtime_hera.dsl import step\n",
    "from hera.workflows import DAG, Workflow\n",
    "\n",
    "\n",
    "def pipeline():\n",
    "    with Workflow(entrypoint=\"dag\") as w:\n",
    "        with DAG(name=\"dag\"):\n",
    "            A = step(\n",
    "                template={\n",
    "                    \"action\": \"build\",\n",
    "                    \"instructions\": [\n",
    "                        \"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\n",
    "                        \"pip3 install darts==0.30.0 patsy scikit-learn\",\n",
    "                    ],\n",
    "                },\n",
    "                function=\"train-time-series-model\",\n",
    "            )\n",
    "            B = step(\n",
    "                template={\n",
    "                    \"action\": \"build\",\n",
    "                    \"instructions\": [\n",
    "                        \"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\n",
    "                        \"pip3 install darts==0.30.0 patsy scikit-learn\",\n",
    "                    ],\n",
    "                },\n",
    "                function=\"serve-time-series-model\",\n",
    "            )\n",
    "            C = step(\n",
    "                template={\"action\": \"job\"},\n",
    "                function=\"train-time-series-model\",\n",
    "                outputs=[\"model\"],\n",
    "            )\n",
    "            D = step(\n",
    "                template={\"action\": \"serve\", \"init_parameters\": {\"model_key\": \"{{inputs.parameters.model}}\"}},\n",
    "                function=\"serve-time-series-model\",\n",
    "                inputs={\"model\": C.get_parameter(\"model\")},\n",
    "            )\n",
    "            [A, B] >> C >> D\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2aa24",
   "metadata": {},
   "source": [
    "### Execute the Complete Pipeline\n",
    "\n",
    "Finally, let's create and execute our complete time series pipeline workflow. This will run environment setup, training, and serving deployment in an automated, orchestrated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.new_workflow(\n",
    "    name=\"time-series-pipeline\",\n",
    "    kind=\"hera\",\n",
    "    code_src=\"src/pipeline.py\",\n",
    "    handler=\"pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140910eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_build = workflow.run(\"build\", wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4571009",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_run = workflow.run(\"pipeline\", wait=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
