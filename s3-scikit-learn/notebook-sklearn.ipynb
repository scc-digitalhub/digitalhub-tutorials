{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebfad8d",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end machine learning pipeline using Scikit-learn with the DigitalHub SDK. We'll work with the breast cancer dataset, train a classification model, deploy it as a REST API service, and orchestrate the entire process.\n",
    "\n",
    "## Overview\n",
    "- **Data Preparation**: Generate and prepare the breast cancer dataset\n",
    "- **Model Training**: Train an SVM classifier with performance metrics\n",
    "- **Model Serving**: Deploy the trained model as a REST API endpoint\n",
    "- **Orchestrate**: Create a workflow pipeline to automate the ML process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950988f",
   "metadata": {},
   "source": [
    "## Setup and Function Definitions\n",
    "\n",
    "First, we'll create the necessary directory structure and define all the functions we'll need for our ML pipeline. All functions will be stored in a single `src/functions.py` file for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158133c",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our main functions file with the following components:\n",
    "\n",
    "- **`data_generator`**: Generates the breast cancer dataset from scikit-learn\n",
    "- **`train_model`**: Trains an SVM classifier and logs performance metrics\n",
    "\n",
    "Each function is decorated with `@handler` to integrate with the DigitalHub runtime system. The training function also logs comprehensive metrics including accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/functions.py\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pickle import dump\n",
    "import sklearn.metrics\n",
    "from digitalhub_runtime_python import handler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    A function which generates the breast cancer dataset from scikit-learn\n",
    "    \"\"\"\n",
    "    breast_cancer = load_breast_cancer()\n",
    "    breast_cancer_dataset = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"target\"])\n",
    "    breast_cancer_dataset = pd.concat([breast_cancer_dataset, breast_cancer_labels], axis=1)\n",
    "    return breast_cancer_dataset\n",
    "\n",
    "\n",
    "@handler(outputs=[\"model\"])\n",
    "def train_model(project, di):\n",
    "    \"\"\"\n",
    "    Train an SVM classifier on the breast cancer dataset and log metrics\n",
    "    \"\"\"\n",
    "    df_cancer = di.as_df()\n",
    "    X = df_cancer.drop([\"target\"], axis=1)\n",
    "    y = df_cancer[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    y_predict = svc_model.predict(X_test)\n",
    "\n",
    "    if not os.path.exists(\"model\"):\n",
    "        os.makedirs(\"model\")\n",
    "\n",
    "    with open(\"model/breast_cancer_classifier.pkl\", \"wb\") as f:\n",
    "        dump(svc_model, f, protocol=5)\n",
    "\n",
    "    metrics = {\n",
    "        \"f1_score\": sklearn.metrics.f1_score(y_test, y_predict),\n",
    "        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_predict),\n",
    "        \"precision\": sklearn.metrics.precision_score(y_test, y_predict),\n",
    "        \"recall\": sklearn.metrics.recall_score(y_test, y_predict),\n",
    "    }\n",
    "    model = project.log_model(name=\"breast_cancer_classifier\", kind=\"sklearn\", source=\"./model/\")\n",
    "    model.log_metrics(metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38a68d",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Now we'll initialize our DigitalHub project using consistent naming with other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfe6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "p_name = \"tutorial-project\"\n",
    "project = dh.get_or_create_project(p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40b49b",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "First step of our ML pipeline - we'll create and run the data preparation function to generate the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_fn = project.new_function(\n",
    "    name=\"prepare-data\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"data_generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46279b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_run = data_gen_fn.run(\"job\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1376f9",
   "metadata": {},
   "source": [
    "Let's examine the generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_data_run.output(\"dataset\")\n",
    "dataset.as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d3d5e",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "Now we'll train our SVM classifier on the breast cancer dataset. The training function will split the data, train the model, and log comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = project.new_function(\n",
    "    name=\"train-classifier\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"train_model\",\n",
    "    requirements=[\"numpy<2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e853073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = train_fn.run(action=\"job\", inputs={\"di\": dataset.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d047dc",
   "metadata": {},
   "source": [
    "## Step 3: Model Serving\n",
    "\n",
    "Now we'll deploy our trained model as a REST API service. This will allow us to make predictions via HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_run.output(\"model\")\n",
    "serve_func = project.new_function(\n",
    "    name=\"serve-classifier\",\n",
    "    kind=\"sklearnserve\",\n",
    "    path=model.spec.path + \"breast_cancer_classifier.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\"serve\", labels=[\"ml-service\"], wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3a2d1",
   "metadata": {},
   "source": [
    "### Test the Model API\n",
    "\n",
    "Let's test our deployed model by making a prediction request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb072f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate sample data for prediction\n",
    "data = np.random.rand(2, 30).tolist()\n",
    "json_payload = {\"inputs\": [{\"name\": \"input-0\", \"shape\": [2, 30], \"datatype\": \"FP32\", \"data\": data}]}\n",
    "\n",
    "# Make prediction\n",
    "result = serve_run.refresh().invoke(json=json_payload).json()\n",
    "print(\"Prediction result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e6b55",
   "metadata": {},
   "source": [
    "## Pipeline Orchestration\n",
    "\n",
    "Now let's create a workflow that orchestrates all the ML steps automatically. This pipeline uses Hera (Argo Workflows) to define the execution flow:\n",
    "\n",
    "1. **A**: Prepare data (generate dataset)\n",
    "2. **B**: Train model (depends on A)\n",
    "\n",
    "The pipeline creates a simple sequential flow where model training depends on data preparation completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da52eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/pipeline.py\"\n",
    "from digitalhub_runtime_hera.dsl import step\n",
    "from hera.workflows import DAG, Workflow\n",
    "\n",
    "\n",
    "def pipeline():\n",
    "    with Workflow(entrypoint=\"dag\") as w:\n",
    "        with DAG(name=\"dag\"):\n",
    "            A = step(template={\"action\": \"job\"},\n",
    "                     function=\"prepare-data\",\n",
    "                     outputs=[\"dataset\"])\n",
    "            B = step(template={\"action\": \"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n",
    "                     function=\"train-classifier\",\n",
    "                     inputs={\"di\": A.get_parameter(\"dataset\")})\n",
    "            A >> B\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb9538",
   "metadata": {},
   "source": [
    "### Execute the Complete Pipeline\n",
    "\n",
    "Finally, let's create and execute our complete ML pipeline workflow. This will run data preparation and model training in an automated, orchestrated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.new_workflow(name=\"ml-pipeline\", kind=\"hera\", code_src=\"src/pipeline.py\", handler=\"pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_run = workflow.run(\"build\", wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3115b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_run = workflow.run(\"pipeline\", wait=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
