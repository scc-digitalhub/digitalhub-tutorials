{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18707eea-1d44-45ef-975d-f59ae55fd00e",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation tutorial\n",
    "\n",
    "In this scenario, we create a *Retrieval-Augmented Generation* (RAG) application, a chatbot able to take new documents (such as PDF files), learn from their contents and answer questions related to them.\n",
    "\n",
    "The steps will be as follows:\n",
    "\n",
    "- Prepare a LLM model\n",
    "- Extract text from a PDF file and generate embeddings\n",
    "- Prepare the RAG application\n",
    "- Provide a UI for the application\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30cb97a0-9576-4169-a93f-30d30f48968f",
   "metadata": {},
   "source": [
    "## Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4484ff4-dd37-452a-bf5c-8ff1ba69c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "import getpass as gt\n",
    "\n",
    "USERNAME = gt.getuser()\n",
    "\n",
    "project = dh.get_or_create_project(f\"{USERNAME}-tutorial-project\")\n",
    "print(project.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b02c5e9-b26b-462f-92b9-fab88f4194ce",
   "metadata": {},
   "source": [
    "# 1. LLM for text generation\n",
    "\n",
    "We'll create a function to serve the LLama3.2 model directly. The model path may use different protocols, such as `ollama://` or `hf://`, to directly reference models from the corresponding hub, without manual downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76bbe42-f8dd-48ed-bbe1-db2a838a7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function = project.new_function(\n",
    "    name=\"llama32-1b\",\n",
    "    kind=\"kubeai-text\",\n",
    "    model_name=f\"{USERNAME}-model\",\n",
    "    url=\"ollama://llama3.2:1b\",\n",
    "    engine='OLlama',\n",
    "    features=['TextGeneration']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5db5057c-bda3-48b3-9bb0-0320d8dadeed",
   "metadata": {},
   "source": [
    "To deploy the model, we use a GPU profile (`1xa100`) to accelerate the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972524e-68b1-4c91-b7e8-04a3b20b6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run = llm_function.run(\"serve\", profile=\"1xa100\", wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbd38722-40fa-42ab-8514-bc46ec62ebcb",
   "metadata": {},
   "source": [
    "Let's check that our service is running and ready to accept requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484f873-645d-4072-8bd4-87d06c53cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = llm_run.refresh().status.service\n",
    "print(\"Service status:\", service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9210a49-07fb-46fa-8006-93c245725ba5",
   "metadata": {},
   "source": [
    "When the service is ready, we need to wait for the model to be downloaded and deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363995aa-f879-4a80-9537-85b1bf2fc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = llm_run.refresh().status.k8s.get(\"Model\")['status']\n",
    "print(\"Model status:\", status)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ec6c9cb-3924-41c6-8323-79af184a4dfb",
   "metadata": {},
   "source": [
    "Once ready, we save the URL and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c1623-90f4-442e-94be-345968e2f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_URL = llm_run.status.to_dict()[\"service\"][\"url\"]\n",
    "CHAT_MODEL = llm_run.status.to_dict()[\"openai\"][\"model\"]\n",
    "print(f\"service {CHAT_URL} with model {CHAT_MODEL}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7665119a-fd7c-4c18-aab9-3423884e5acc",
   "metadata": {},
   "source": [
    "## Test the LLM API\n",
    "\n",
    "Let's test our deployed model with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421aa49-e08e-46cd-938b-db2ce7e490c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =llm_run.refresh().status.k8s.get(\"Model\").get(\"metadata\").get(\"name\")\n",
    "json_payload = {'model': model_name, 'prompt': 'Describe MLOps'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47f1dc-f707-4de4-ab82-df435b3c42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "result = llm_run.invoke(model_name=model_name, json=json_payload, url=service['url']+'/v1/completions').json()\n",
    "print(\"Response:\")\n",
    "pp.pprint(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5881b3c-8363-4b35-b014-93faa7765ba8",
   "metadata": {},
   "source": [
    "The response contains the answer, as well as some usage parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeee9ad4-f86f-4cbc-b701-aff3e5df4779",
   "metadata": {},
   "source": [
    "# 2. Building a knowledge base\n",
    "\n",
    "We now define the process to extract text content from the PDF file and generate embeddings from it.\n",
    "\n",
    "## Text extraction\n",
    "\n",
    "### Deploy a text extraction service\n",
    "\n",
    "We will use [Apache Tika](https://tika.apache.org/), a tool for extracting text from a variety of formats. Create the function, run it and obtain the URL of the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fc523-53d1-4ba1-bd7a-86204606986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tika_function = project.new_function(\"tika\", kind=\"container\", image=\"apache/tika:latest-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cf20b-d95f-4fcf-9061-3c78ac049e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tika_run = tika_function.run(\"serve\", service_ports = [{\"port\": 9998, \"target_port\": 9998}], wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99f87c-55e0-4ea7-8d5c-84c68d2bf4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = tika_run.refresh().status.service\n",
    "print(\"Service status:\", service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b38a28-969b-4b5e-bc5d-cca12b020587",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIKA_URL = tika_run.status.to_dict()[\"service\"][\"url\"]\n",
    "print(TIKA_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a6baa30-1657-46b4-9957-7d359b35f52c",
   "metadata": {},
   "source": [
    "### Extract the text\n",
    "\n",
    "We create a python function which will read an artifact from the platform's repository and leverage the Tika service to extract the textual content and write it to a HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716f397-9073-461d-aa34-05815083cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_function = project.new_function(\n",
    "    name=\"extract\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/extract.py\",\n",
    "    handler=\"extract_text\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3e4af8f-5e51-4d85-8261-0b55a5f8e04c",
   "metadata": {},
   "source": [
    "We store the PDF file as artifact and download it. You are free to change the address to whichever PDF file you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf4889-86d4-46c6-968f-b7720f501b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = project.new_artifact(\"document.pdf\",kind=\"artifact\", path=\"https://harvard-ml-courses.github.io/cs181-web-2024/static/cs181-textbook.pdf\")\n",
    "pdf.download(\"document.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd2462b-559d-4621-ba2c-08ba40e2fe94",
   "metadata": {},
   "source": [
    "Then, we run the function by passing it the artifact and the URL to Tika:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9e5bd-f14f-446a-af9a-8574b19880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_run = extract_function.run(\"job\", inputs={\"artifact\": pdf.key}, parameters={\"tika_url\": TIKA_URL}, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1d02748-55d8-410e-8481-a7ce84ac210b",
   "metadata": {},
   "source": [
    "Let's read the file and check the content is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc176e-a4aa-4819-a04d-fe828e4c05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_artifact = project.get_artifact(\"document.pdf_output.html\")\n",
    "html_artifact.download()\n",
    "with open('./artifact/output.html', 'r') as file:\n",
    "    file_content = file.read()\n",
    "    print(file_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55857e96-ebd7-4ab0-b096-7143da5e1af9",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are vectors of floating-point numbers that represent words and indicate how strong the connection between certain words is.\n",
    "\n",
    "We need to deploy a suitable model to generate embeddings from the extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ba934-e482-475f-a8b0-3f8909aefa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_function = project.new_function(\n",
    "    \"embed\",\n",
    "    kind=\"kubeai-text\",\n",
    "    model_name=\"embmodel\",\n",
    "    features=[\"TextEmbedding\"],\n",
    "    engine=\"VLLM\",\n",
    "    url=\"hf://thenlper/gte-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729391df-873c-4b95-a850-804f2b95bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_run = embed_function.run(\"serve\", wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc7fdd-2562-4058-ae96-2845871c4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = embed_run.refresh().status\n",
    "print(\"Service status:\", status.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fcc67-0d21-46d8-bf50-22c7a7db180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_URL = status.to_dict()[\"service\"][\"url\"]\n",
    "EMBED_MODEL = status.to_dict()[\"openai\"][\"model\"]\n",
    "print(f\"service {EMBED_URL} with model {EMBED_MODEL}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebe06123-42ca-4480-97f7-27640626c40a",
   "metadata": {},
   "source": [
    "Let's check that the model is ready. We need the OpenAI client installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0536ab-6d29-48df-86e4-9cf7c896daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada8835-06a4-4d4d-a6b2-b78963538ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ignored\", base_url=f\"{EMBED_URL}/v1\")\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text goes here.\",\n",
    "    model=EMBED_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bf808-dc35-4fb5-813d-dfa626c52d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e36358b6-6f7a-4574-8f6f-57fb4c11579c",
   "metadata": {},
   "source": [
    "### Embedding generation\n",
    "We define a function to read the text from the repository and push the data into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc4049-1c36-4fd0-833b-f6acea5a629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_function = project.new_function(\n",
    "    name=\"embedder\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    requirements=[\n",
    "        \"transformers==4.50.3\",\n",
    "        \"psycopg_binary\",\n",
    "        \"openai\",\n",
    "        \"langchain-text-splitters\",\n",
    "        \"langchain-community\",\n",
    "        \"langgraph\",\n",
    "        \"langchain-core\",\n",
    "        \"langchain-huggingface\",\n",
    "        \"langchain_postgres\",\n",
    "        \"langchain[openai]\",\n",
    "        \"beautifulsoup4\",\n",
    "    ],\n",
    "    code_src=\"src/embedder.py\",\n",
    "    handler=\"process\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b936236b-6449-40ed-86d8-0b405c9506b4",
   "metadata": {},
   "source": [
    "Parameters are as follows:\n",
    "\n",
    "- Embed model is served at `EMBED_URL` with `EMBED_MODEL`.\n",
    "- Input artifact (HTML) is `html_artifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1f1e4-9ec1-44a4-8979-fdee70d51df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_run = embedder_function.run(\n",
    "    \"job\",\n",
    "    inputs={\"input\": html_artifact.key},\n",
    "    envs=[\n",
    "        {\n",
    "            \"name\": \"EMBEDDING_SERVICE_URL\",\n",
    "            \"value\": EMBED_URL\n",
    "        },\n",
    "        {    \"name\": \"EMBEDDING_MODEL_NAME\",\n",
    "            \"value\": EMBED_MODEL,\n",
    "        }\n",
    "    ],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0fba758-fd61-4569-8a1a-7af902d45727",
   "metadata": {},
   "source": [
    "Check that the run has completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88442c7-da20-4b15-a149-1ec08ea7f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_run.status.state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25f57b12-1ec2-4f2c-83bb-c0ca8bda8df6",
   "metadata": {},
   "source": [
    "# 3. RAG application with LangChain\n",
    "\n",
    "This step will define the agent which connects the embedding model, the chat model and the vector store to fullfill the RAG scenario.\n",
    "\n",
    "You should have the URLs and models for the latest `RUNNING` runs of the two functions from the previous steps of the scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc81e5-6324-447e-993f-6ac158d08f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Service {EMBED_URL} with model {EMBED_MODEL}\")\n",
    "print(f\"Service {CHAT_URL} with model {CHAT_MODEL}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "942aca50-e208-4029-b34b-ec54ce896493",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "\n",
    "We will register a python function implementing the RAG agent with [LangChain](https://python.langchain.com/docs/introduction/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e93c0d-8c39-48fc-b8c3-a20f722fd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_func = project.new_function(\n",
    "    name=\"rag-service\", \n",
    "    kind=\"python\", \n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/serve.py\",     \n",
    "    handler=\"serve\",\n",
    "    init_function=\"init\",\n",
    "    requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e2c2184-f1cf-4f6e-89df-1862a5793bdf",
   "metadata": {},
   "source": [
    "Then, we can run an instance connecting the model services together. It may take a while for this run to finish initialization. If the execution fails, it is probably due to the large number of dependencies required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61711116-73c5-4151-8de3-4a68c9266fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\n",
    "    action=\"serve\",\n",
    "    resources={\n",
    "        \"cpu\": {\"limits\": \"8\", \"requests\": \"4\"},\n",
    "        \"mem\": {\"limits\": \"8Gi\", \"requests\": \"4Gi\"},\n",
    "    },\n",
    "    envs=[\n",
    "            {\"name\": \"CHAT_MODEL_NAME\", \"value\": CHAT_MODEL},\n",
    "            {\"name\": \"CHAT_SERVICE_URL\", \"value\": CHAT_URL},\n",
    "            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": EMBED_MODEL},\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": EMBED_URL}\n",
    "         ],\n",
    "    secrets=[\"PG_CONN_URL\"],\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c43ff6-4349-442b-97ff-ea3c77489893",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_URL = serve_run.status.to_dict()[\"service\"][\"url\"]\n",
    "print(AGENT_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc3f46a0-4865-41cd-89e8-fe084882cb5b",
   "metadata": {},
   "source": [
    "To test our API, we make a call to the service endpoint, providing JSON text with an example question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28716c87-a78b-4580-8498-265df9b30f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post(f\"http://{AGENT_URL}\",json={\"question\": \"What is the idea behind SVMs?\"})\n",
    "print(res.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df7db771-615e-4c67-873f-88c5881e5872",
   "metadata": {},
   "source": [
    "# 4. Agent Web UI\n",
    "\n",
    "Finally, we build a web interface to test the agent. The interface will be available via browser by proxying the port through the workspace.\n",
    "\n",
    "## Deploy the UI\n",
    "\n",
    "We use [Streamlit](https://docs.streamlit.io/) to serve a simple webpage with an input field connected to the agent API.\n",
    "\n",
    "Streamlit is a Python framework to create browser applications with little code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a43079-1f42-4fea-8329-792e1303abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU streamlit langgraph langchain-core langchain-postgres \"langchain[openai]\" psycopg_binary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7c758a4-2c19-4287-aed9-1e06f30e8f3b",
   "metadata": {},
   "source": [
    "Add the models' names and service URLs to the environment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499b2d9-7aaf-4425-9337-5342ec05f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./streamlit.env\", \"w\") as env_file:\n",
    "    env_file.write(f\"CHAT_MODEL_NAME={CHAT_MODEL}\\n\")\n",
    "    env_file.write(f\"CHAT_SERVICE_URL={CHAT_URL}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_MODEL_NAME={EMBED_MODEL}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_SERVICE_URL={EMBED_URL}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c479fcf-8706-4b45-98b8-b1f8b3916987",
   "metadata": {},
   "source": [
    "Write the function implementing the RAG UI to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3b0d2-75b0-43c8-b34e-dca355949b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'rag-streamlit-app.py'\n",
    "import os\n",
    "import bs4\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langgraph.graph import START, StateGraph\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Read environment variables\n",
    "add_env_path = Path('.') / 'streamlit.env'\n",
    "load_dotenv(dotenv_path=add_env_path, override=True)\n",
    "\n",
    "PG_USER = os.environ[\"DB_USERNAME\"]\n",
    "PG_PASS = os.environ[\"DB_PASSWORD\"]\n",
    "PG_HOST = os.environ[\"DB_HOST\"]\n",
    "PG_PORT = os.environ[\"DB_PORT\"]\n",
    "DB_NAME = os.environ[\"DB_DATABASE\"]\n",
    "ACCESS_TOKEN = os.environ[\"DHCORE_ACCESS_TOKEN\"]\n",
    "\n",
    "chat_model_name = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "chat_service_url = os.environ[\"CHAT_SERVICE_URL\"]\n",
    "embedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "embedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\n",
    "PG_CONN_URL = (\n",
    "    f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "class CEmbeddings(OpenAIEmbeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n",
    "        emb_arr = []\n",
    "        for doc in docs:\n",
    "            #sanitize string: replace NUL with spaces\n",
    "            d=doc.replace(\"\\x00\", \"-\")            \n",
    "            embs = client.embeddings.create(\n",
    "                input=d,\n",
    "                model=embedding_model_name\n",
    "            )\n",
    "            emb_arr.append(embs.data[0].embedding)\n",
    "        return emb_arr\n",
    "\n",
    "custom_embeddings = CEmbeddings(api_key=\"ignored\")\n",
    "\n",
    "# Vector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=custom_embeddings,\n",
    "    collection_name=f\"{embedding_model_name}_docs\",\n",
    "    connection=PG_CONN_URL,\n",
    ")\n",
    "\n",
    "# Chat model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ignore\"\n",
    "llm = init_chat_model(chat_model_name, model_provider=\"openai\", base_url=f\"{chat_service_url}/v1/\")\n",
    "\n",
    "# Define prompt and operations\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Define graph of operations\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Streamlit setup\n",
    "st.title(\"RAG App\")\n",
    "st.write(\"Welcome to the RAG (Retrieval-Augmented Generation) app.\")\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "qa = st.container()\n",
    "\n",
    "with st.form(\"rag_form\", clear_on_submit=True):\n",
    "    question = st.text_input(\"Question\", \"\")\n",
    "    submit = st.form_submit_button(\"Submit\")\n",
    "    \n",
    "if submit:\n",
    "    # Load and chunk contents\n",
    "    if question:\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        with qa.chat_message(\"user\"):\n",
    "            st.write(question)\n",
    "    \n",
    "        response = graph.invoke({\"question\": question})\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(response[\"answer\"])\n",
    "    else:\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(\"You didn't provide a question!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a0332bd-5e6b-41ea-a0e4-2ff9a5de1fe9",
   "metadata": {},
   "source": [
    "## Launch and test the Streamlit app\n",
    "\n",
    "This command launches the Streamlit app, based on the file written by the previous cell. To access the app, you will need to [forward port 8501 in Coder](https://scc-digitalhub.github.io/docs/tasks/workspaces/#port-forwarding).\n",
    "\n",
    "Try asking the app a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7113657-0250-4eea-923a-6b693c94cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run rag-streamlit-app.py --browser.gatherUsageStats false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OltreAI)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
