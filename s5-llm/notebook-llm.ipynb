{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2e0e6c",
   "metadata": {},
   "source": [
    "# Large Language Model Serving Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to deploy and serve a pre-trained Large Language Model using HuggingFace transformers with the DigitalHub SDK. We'll work with a DistilBERT model for sentiment classification and deploy it as a REST API service.\n",
    "\n",
    "## Overview\n",
    "- **Model Selection**: Use a pre-trained DistilBERT model from HuggingFace Hub\n",
    "- **Model Serving**: Deploy the model as a REST API endpoint with GPU acceleration\n",
    "- **Inference**: Test the deployed model with text classification tasks\n",
    "- **Integration**: Seamless integration with DigitalHub's serving infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28de18d",
   "metadata": {},
   "source": [
    "## Setup and Model Configuration\n",
    "\n",
    "We'll set up our DigitalHub project and configure the HuggingFace model for serving. The model we'll use is DistilBERT fine-tuned for sentiment classification (SST-2 dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079a02d",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Now we'll initialize our DigitalHub project using consistent naming with other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "p_name = \"tutorial-project\"\n",
    "project = dh.get_or_create_project(p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493dc86",
   "metadata": {},
   "source": [
    "## Step 1: Model Configuration\n",
    "\n",
    "We'll create a function to serve the DistilBERT model directly from HuggingFace Hub. This model is fine-tuned for sentiment classification and can classify text as positive or negative.\n",
    "\n",
    "The model path uses the `huggingface://` protocol to directly reference models from the HuggingFace Hub without manual downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function = project.new_function(\n",
    "    name=\"sentiment-classifier\",\n",
    "    kind=\"huggingfaceserve\",\n",
    "    model_name=\"sentiment-model\",\n",
    "    path=\"huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda90a8",
   "metadata": {},
   "source": [
    "## Step 2: Model Serving\n",
    "\n",
    "Now we'll deploy our LLM model as a REST API service. We're using a GPU profile (`1xa100`) to accelerate inference for better performance with the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run = llm_function.run(\"serve\", profile=\"1xa100\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3e7d5",
   "metadata": {},
   "source": [
    "Let's check that our service is running and ready to accept requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = llm_run.refresh().status.service\n",
    "print(\"Service status:\", service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337f341",
   "metadata": {},
   "source": [
    "### Test the LLM API\n",
    "\n",
    "Now let's test our deployed sentiment classification model with some sample text. We'll send both positive and negative sentiment examples to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for sentiment classification\n",
    "model_name = \"sentiment-model\"\n",
    "json_payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input-0\",\n",
    "            \"shape\": [2],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"],\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction request to the deployed LLM\n",
    "result = llm_run.invoke(model_name=model_name, json=json_payload).json()\n",
    "print(\"Sentiment classification results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614da78",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "The model returns sentiment predictions for each input text:\n",
    "- **Positive sentiment**: Higher probability for positive class\n",
    "- **Negative sentiment**: Higher probability for negative class\n",
    "\n",
    "The DistilBERT model has been fine-tuned on the Stanford Sentiment Treebank (SST-2) dataset, making it effective for binary sentiment classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a0b05",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have successfully:\n",
    "\n",
    "1. **Deployed a Pre-trained LLM**: Used DistilBERT from HuggingFace Hub for sentiment classification\n",
    "2. **GPU-Accelerated Serving**: Deployed the model with GPU acceleration for optimal performance  \n",
    "3. **REST API Integration**: Created a REST endpoint for real-time sentiment analysis\n",
    "4. **Tested the Service**: Verified the model works correctly with sample text inputs\n",
    "\n",
    "The LLM is now ready to handle sentiment classification requests through the DigitalHub serving infrastructure. The service can be integrated into applications, workflows, or used for batch processing tasks.\n",
    "\n",
    "This approach demonstrates how easy it is to deploy state-of-the-art language models using DigitalHub's HuggingFace integration, without needing to manage model downloads, dependencies, or serving infrastructure manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19055e2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have successfully:\n",
    "\n",
    "1. **Deployed a Pre-trained LLM**: Used DistilBERT from HuggingFace Hub for sentiment classification\n",
    "2. **GPU-Accelerated Serving**: Deployed the model with GPU acceleration for optimal performance  \n",
    "3. **REST API Integration**: Created a REST endpoint for real-time sentiment analysis\n",
    "4. **Tested the Service**: Verified the model works correctly with sample text inputs\n",
    "\n",
    "The LLM is now ready to handle sentiment classification requests through the DigitalHub serving infrastructure. The service can be integrated into applications, workflows, or used for batch processing tasks.\n",
    "\n",
    "This approach demonstrates how easy it is to deploy state-of-the-art language models using DigitalHub's HuggingFace integration, without needing to manage model downloads, dependencies, or serving infrastructure manually."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
