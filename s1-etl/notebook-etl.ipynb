{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54f6869",
   "metadata": {},
   "source": [
    "# ETL Pipeline Tutorial with DigitalHub\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end ETL (Extract, Transform, Load) pipeline using the DigitalHub SDK. We'll work with Bologna's traffic data from open data APIs, process it through multiple stages, and deploy it as a REST API service.\n",
    "\n",
    "## Overview\n",
    "- **Extract**: Download traffic data from Bologna's open data portal\n",
    "- **Transform**: Process and clean the data in multiple stages\n",
    "- **Load**: Deploy the processed data as a queryable REST API\n",
    "- **Orchestrate**: Create a workflow pipeline to automate the entire process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78a911",
   "metadata": {},
   "source": [
    "## Setup and Function Definitions\n",
    "\n",
    "First, we'll create the necessary directory structure and define all the functions we'll need for our ETL pipeline. All functions will be stored in a single `src/functions.py` file for easy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"src\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468c229",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "This cell creates our main functions file with the following components:\n",
    "\n",
    "- **`downloader`**: Extracts CSV data from the Bologna open data API\n",
    "- **`process_spire`**: Transforms the raw data to extract unique traffic sensor information\n",
    "- **`process_measures`**: Reshapes time-series traffic measurements from wide to long format\n",
    "- **`init_context` & `serve`**: API serving functions for deploying the processed data as a REST endpoint\n",
    "\n",
    "Each function is decorated with `@handler` to integrate with the DigitalHub runtime system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaafa6a-e8c5-41a6-b5b1-822e03a6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/functions.py\"\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "\n",
    "\n",
    "COLS=['codice spira','longitudine','latitudine',\n",
    "      'Livello','tipologia','codice','codice arco',\n",
    "      'codice via','Nome via', 'stato','direzione',\n",
    "      'angolo','geopoint']\n",
    "\n",
    "KEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00',\n",
    "        '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00',\n",
    "        '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00',\n",
    "        '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00',\n",
    "        '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00',\n",
    "        '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\n",
    "COLUMNS=['data','codice spira']\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(url):\n",
    "    return url.as_df(file_format='csv',sep=\";\")\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset-spire\"])\n",
    "def process_spire(di):\n",
    "    df = di.as_df()\n",
    "    return df.groupby(['codice spira']).first().reset_index()[COLS]\n",
    "\n",
    "\n",
    "@handler(outputs=[\"dataset-measures\"])\n",
    "def process_measures(di):\n",
    "    df = di.as_df()\n",
    "    rdf = df[COLUMNS+KEYS]\n",
    "    ls = []\n",
    "    for key in KEYS:\n",
    "        k = key.split(\"-\")[0]\n",
    "        xdf = rdf[COLUMNS + [key]]\n",
    "        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n",
    "        xdf['value'] = xdf[key]\n",
    "        ls.append(xdf[['time','codice spira','value']])\n",
    "    return pd.concat(ls)\n",
    "\n",
    "\n",
    "def init_context(context, dataitem):\n",
    "    di = context.project.get_dataitem(dataitem)\n",
    "    df = di.as_df()\n",
    "    setattr(context, \"df\", df)\n",
    "\n",
    "\n",
    "def serve(context, event):\n",
    "    df = context.df\n",
    "\n",
    "    if df is None:\n",
    "        return \"\"\n",
    "\n",
    "    # mock REST api\n",
    "    fields = event.fields\n",
    "\n",
    "    # pagination\n",
    "    page = 0\n",
    "    pageSize = 50\n",
    "\n",
    "    if \"page\" in fields:\n",
    "        page = int(fields[\"page\"])\n",
    "\n",
    "    if \"size\" in fields:\n",
    "        pageSize = int(fields[\"size\"])\n",
    "\n",
    "    if page < 0:\n",
    "        page = 0\n",
    "\n",
    "    if pageSize < 1:\n",
    "        pageSize = 1\n",
    "\n",
    "    if pageSize > 100:\n",
    "        pageSize = 100\n",
    "\n",
    "    start = page * pageSize\n",
    "    end = start + pageSize\n",
    "    total = len(df)\n",
    "\n",
    "    if end > total:\n",
    "        end = total\n",
    "\n",
    "    ds = df.iloc[start:end]\n",
    "    json = ds.to_json(orient=\"records\")\n",
    "\n",
    "    return {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa0dbb",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "Now we'll initialize our DigitalHub project. This creates a workspace where we can manage our data items, functions, workflows, and deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6cab4-a33e-4682-8629-083e483b68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "p_name = \"tutorial-project\"\n",
    "project = dh.get_or_create_project(p_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37068e25",
   "metadata": {},
   "source": [
    "## Data Source Setup\n",
    "\n",
    "We'll create a data item that points to Bologna's traffic flow data. This dataset contains hourly vehicle counts from traffic sensors (spire) throughout the city for 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925a8de-f320-4bce-880b-9c7f21bb70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&timezone=Europe%2FRome&use_labels=true&delimiter=%3B\"\n",
    "di = project.new_dataitem(name=\"url-data-item\", kind=\"table\", path=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679df2",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "First step of our ETL pipeline - we'll create and run the `download-data` function to extract the raw CSV data from the Bologna open data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc122e5-0823-4c02-af3d-5a021b35e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = project.new_function(\n",
    "    name=\"download-data\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"downloader\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e5dbf-bfc1-49b6-8fc6-a00e88e78245",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = func.run(\"job\", inputs={\"url\": di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1e1bb",
   "metadata": {},
   "source": [
    "Let's examine the raw data we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6a2bb-eadf-4397-85dd-fff1c2dd76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_di = project.get_dataitem(\"dataset\")\n",
    "dataset_di.as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a1c6f",
   "metadata": {},
   "source": [
    "## Step 2: Transform - Process Traffic Sensors (Spire)\n",
    "\n",
    "The first transformation extracts unique traffic sensor metadata. Since the raw data contains many duplicate sensor records, we'll group by sensor code and keep only the first occurrence to get clean sensor information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260e03b-13ee-42d5-8980-7d096f88d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_func = project.new_function(\n",
    "    name=\"process-spire\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"process_spire\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d4ed4-cc2e-4106-b7ef-0f9f382487f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_run = process_func.run(\"job\", inputs={\"di\": dataset_di.key}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9341db4",
   "metadata": {},
   "source": [
    "Let's see the cleaned sensor data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e57ec3-e276-4bee-a75a-539d4d09e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_run.output(\"dataset-spire\").as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9a26d",
   "metadata": {},
   "source": [
    "## Step 3: Transform - Process Traffic Measurements\n",
    "\n",
    "The second transformation reshapes the hourly traffic data from wide format (24 hour columns) to long format (time-value pairs). This makes the data more suitable for analysis and API queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824c93b-e6f1-4316-b41b-bffe9a660c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_measures_func = project.new_function(\n",
    "    name=\"process-measures\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"process_measures\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f7218-eb0d-4976-a5c6-fa9183e49eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_measures_run = process_measures_func.run(\n",
    "    \"job\", inputs={\"di\": dataset_di.key}, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee04a77",
   "metadata": {},
   "source": [
    "Let's examine the transformed time-series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39565c-d220-4214-a46e-84b9aa9f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_meas = process_measures_run.output(\"dataset-measures\")\n",
    "di_meas.as_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a21002",
   "metadata": {},
   "source": [
    "## Step 4: Load - Deploy as REST API\n",
    "\n",
    "Now we'll deploy our processed data as a REST API service. The API will support pagination and allow querying the traffic measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e72d61-c896-4e7a-9ba4-f4b3a078f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_func = project.new_function(\n",
    "    name=\"api\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/functions.py\",\n",
    "    handler=\"serve\",\n",
    "    init_function=\"init_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041540f3-3a6e-4a99-8474-b0f9b4c1c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_serve_model = api_func.run(\n",
    "    \"serve\", init_parameters={\"dataitem\": di_meas.key}, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101db9f7",
   "metadata": {},
   "source": [
    "### Test the API\n",
    "\n",
    "Let's test our deployed API by making a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796afbe4-1b14-45b2-8a87-cd89f4fb2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "svc_url = f\"http://{run_serve_model.status.service['url']}/?page=5&size=10\"\n",
    "res = run_serve_model.invoke(url=svc_url).json()\n",
    "rdf = pd.read_json(res[\"data\"], orient=\"records\")\n",
    "rdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59accb31",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38b8d5",
   "metadata": {},
   "source": [
    "Now let's create a workflow that orchestrates all these steps automatically. This pipeline uses Hera (Argo Workflows) to define the execution flow:\n",
    "\n",
    "1. **A**: Download data\n",
    "2. **B**: Process sensors (depends on A)\n",
    "3. **C**: Process measurements (depends on A) \n",
    "4. **D**: Deploy API (depends on C)\n",
    "\n",
    "The pipeline creates a DAG (Directed Acyclic Graph) where steps can run in parallel when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/pipeline.py\"\n",
    "from hera.workflows import Workflow, DAG, Parameter\n",
    "from digitalhub_runtime_hera.dsl import step\n",
    "\n",
    "\n",
    "def pipeline():\n",
    "    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"url\")) as w:\n",
    "\n",
    "        with DAG(name=\"dag\"):\n",
    "            A = step(template={\"action\":\"job\", \"inputs\": {\"url\": \"{{workflow.parameters.url}}\"}},\n",
    "                     function=\"download-data\",\n",
    "                     outputs=[\"dataset\"])\n",
    "            B = step(template={\"action\":\"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n",
    "                     function=\"process-spire\",\n",
    "                     inputs={\"di\": A.get_parameter(\"dataset\")})\n",
    "            C = step(template={\"action\":\"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n",
    "                     function=\"process-measures\",\n",
    "                     inputs={\"di\": A.get_parameter(\"dataset\")},\n",
    "                     outputs=[\"dataset-measures\"])\n",
    "            D = step(template={\"action\": \"serve\", \"init_parameters\": {\"dataitem\": \"{{inputs.parameters.dataitem}}\"}},\n",
    "                     function=\"api\",\n",
    "                     inputs={\"dataitem\": C.get_parameter(\"dataset-measures\")})\n",
    "            A >> [B, C]\n",
    "            C >> D\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f2d3f",
   "metadata": {},
   "source": [
    "### Execute the Complete Pipeline\n",
    "\n",
    "Finally, let's create and execute our complete ETL pipeline workflow. This will run all the steps we performed manually above, but in an automated, orchestrated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ef4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.new_workflow(\n",
    "    name=\"pipeline\", kind=\"hera\", code_src=\"src/pipeline.py\", handler=\"pipeline\"\n",
    ")\n",
    "wf_run = workflow.run(\"build\", wait=True)\n",
    "wf_run = workflow.run(\"pipeline\", parameters={\"url\": di.key}, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
