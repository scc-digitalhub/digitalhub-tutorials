{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e7a1652-e4b9-4e96-b3ce-0c52f199c884",
   "metadata": {},
   "source": [
    "## 1. Initialize project\n",
    "Create or use an existing project to scope the RAG scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915e6ea-2cc7-4ba7-aca7-8d88c84f20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "project = dh.get_or_create_project(\"rag-kubeai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e9d74fc-4c28-4538-9734-de640cd7a313",
   "metadata": {},
   "source": [
    "### 1.1. Prepare secrets\n",
    "Two secrets are needed:\n",
    "- `HF_TOKEN`: The HuggingFace token, to use protected HuggingFace models\n",
    "- `PG_CONN_URL`: full PGVector DB URL to connect to the vector store. The value may be obtained from the platform configuration or a new DB may be created from KRM with the necessary extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524cd03-7912-499b-a1eb-c4b0389c77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dotenv -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b29fb-3eeb-463b-b566-5723e782e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path(\".\") / \"rag-kubeai.env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "project.new_secret(\"HF_TOKEN\", secret_value=os.environ[\"HF_TOKEN\"])\n",
    "project.new_secret(\"PG_CONN_URL\", secret_value=os.environ[\"PG_CONN_URL\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4117f5d2-2fb2-4cc8-8671-9047e4499c99",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for text generation\n",
    "For text generation, we deploy the `meta-llama/meta-llama-3-8b-instruct` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d9a77-cdd9-4a7b-b898-d724dc0a6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_func = project.new_function(\"chat\",\n",
    "                                    kind=\"kubeai-text\",\n",
    "                                    model_name=\"chatmodel\",\n",
    "                                    features=[\"TextGeneration\"],\n",
    "                                    url=\"hf://meta-llama/meta-llama-3-8b-instruct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "056c1c33-7c93-42e3-9510-45792610806e",
   "metadata": {},
   "source": [
    "We run the function with the following parameters:\n",
    "- ``profile``: Execution profile for the node selection and resource usage (depends on the platform). In this example, 1xa100 refers to 1 GPU of type A100.\n",
    "- ``max_length``: Length of the context window for the text generation.\n",
    "- ``secrets``: List of secrets to pass to LLM. Needed if HuggingFace token is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf04441-c1eb-4f21-b82a-18aad7f3cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_run = chat_func.run(action=\"serve\",\n",
    "                           profile=\"1xa100\",\n",
    "                           max_length=\"5000\",\n",
    "                           secrets=[\"HF_TOKEN\"],\n",
    "                           wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bb60e2e-bfef-44a4-9bbf-772ff737a3f7",
   "metadata": {},
   "source": [
    "Obtain the name of the deployed model and URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cc14c-b6fa-4511-b789-0302333e3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_name = chat_run.refresh().status.to_dict()[\"openai\"][\"model\"]\n",
    "chat_service_url = chat_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cdccbe5-dbba-43de-a676-dc5fd1d37435",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for embeddings\n",
    "\n",
    "Embedding models map discrete data, such as words, to numerical vectors, which are more convenient for analysis, yet can still represent relationships between objects. We deploy the `thenlper/gte-base` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d24e8-80da-4d90-874f-fcab9f479428",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_func = project.new_function(\"emb\",\n",
    "                                kind=\"kubeai-text\",\n",
    "                                model_name=\"embmodel\",\n",
    "                                features=[\"TextEmbedding\"],\n",
    "                                engine=\"VLLM\",\n",
    "                                url=\"hf://thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a7f9a-f6fa-4c7b-af40-9373c121e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_run = emb_func.run(action=\"serve\",\n",
    "                       wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7361a49c-4e1a-48f1-bcb0-2b226b9f4a23",
   "metadata": {},
   "source": [
    "Obtain the name of the deployed model and URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e3d7d-5587-4229-a2eb-a30571dec65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = emb_run.refresh().status.to_dict()[\"openai\"][\"model\"]\n",
    "embedding_service_url = emb_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56d941f1-1127-474a-9fd9-6b122c9efbc9",
   "metadata": {},
   "source": [
    "## 3. Process the relevant information and store embeddings in the Vector storage\n",
    "\n",
    "In a RAG scenario, a typical task is to store the supporting information into the vector storage and use it later for the text generation. In our example, the relevant information is first scraped from a Web page URL and then stored into the platform using the provided PGVector storage. Two components are required:\n",
    "- Embeddings processor that uses Open Inference Protocol of our embedding model service:\n",
    "  ```python\n",
    "    class CEmbeddings(OpenAIEmbeddings):\n",
    "        def embed_documents(self, docs):\n",
    "            client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n",
    "            emb_arr = []\n",
    "            for doc in docs:\n",
    "                embs = client.embeddings.create(\n",
    "                    input=doc,\n",
    "                    model=embedding_model_name\n",
    "                )\n",
    "                emb_arr.append(embs.data[0].embedding)\n",
    "            return emb_arr\n",
    "\n",
    "    custom_embeddings = CEmbeddings(api_key=\"ignored\")\n",
    "  ```\n",
    "- PGVector storage from the platform:\n",
    "  ```python\n",
    "    vector_store = PGVector(\n",
    "        embeddings=custom_embeddings,\n",
    "        collection_name=\"my_docs\",\n",
    "        connection=os.environ[\"PG_CONN_URL\"],\n",
    "    )\n",
    "  ```\n",
    "\n",
    "We define a Python job to obtain the document, create chunks, and store their embeddings in the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8d321-b0ea-4498-a856-f0deb69777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageurl = \"https://lilianweng.github.io/posts/2023-06-23-agent/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57c785-89a2-4f1c-9a9c-dae76ebf4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_func = project.new_function(\"create-embeddings\", \n",
    "                                   kind=\"python\", \n",
    "                                   python_version=\"PYTHON3_10\",\n",
    "                                   code_src=\"src/embedding.py\",\n",
    "                                   handler=\"embed\",\n",
    "                                   requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08df848-4b26-4755-8e22-6628ad76cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = data_func.run(\n",
    "    action=\"job\", \n",
    "    parameters={\"url\": pageurl},\n",
    "    envs=[\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url},\n",
    "            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": embedding_model_name}\n",
    "        ],\n",
    "    secrets=[\"PG_CONN_URL\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e9f19be-3739-46d4-a417-8dc15d97332f",
   "metadata": {},
   "source": [
    "The results of the elaboration are stored in the corresponding database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f167f465-bf0c-4c83-9269-04d8a4b18fb6",
   "metadata": {},
   "source": [
    "## 4. Create RAG application API\n",
    "Once the components and data are in place, we can create a LangChain-based application and expose it as API in the platform. This will use the chat model service, the vector database, and the serverless functionality.\n",
    "\n",
    "We create and deploy the serverless function that interacts with the LLM and uses the vector store for retrieval. It uses a simple LangChaing graph composed out of two steps: retrieval and generation. The result of the generation is returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ad63-b4dd-4d6a-b524-7214fd975a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_func = project.new_function(\n",
    "    name=\"rag-service\", \n",
    "    kind=\"python\", \n",
    "    python_version=\"PYTHON3_10\", \n",
    "    code_src=\"src/serve.py\",     \n",
    "    handler=\"serve\",\n",
    "    init_function=\"init\",\n",
    "    requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e72dd-b51f-4b2b-8309-50da8901166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\n",
    "    action=\"serve\",\n",
    "    envs=[\n",
    "            {\"name\": \"CHAT_MODEL_NAME\", \"value\": chat_model_name},\n",
    "            {\"name\": \"CHAT_SERVICE_URL\", \"value\": chat_service_url},\n",
    "            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": embedding_model_name},\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url}\n",
    "         ],\n",
    "    secrets=[\"PG_CONN_URL\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7869430-d804-47f9-b8d2-a52f554962c5",
   "metadata": {},
   "source": [
    "To test our API we make a call to the service endpoint providing a JSON with the example question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96ac1c-89dd-4588-9598-ed225e6818ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run.refresh().status.to_dict()[\"service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4ea4a-46f2-4d1c-883c-d665e2d6ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "serve_service_url = serve_run.refresh().status.to_dict()[\"service\"][\"url\"]\n",
    "\n",
    "res = requests.post(f\"http://{serve_service_url}\",json={\"question\": \"What is decomposition in LLM?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742f1c9-d5b9-486d-9097-3a2258849e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77c5e1d5-d57f-4d67-a903-5b9217a84a33",
   "metadata": {},
   "source": [
    "## Streamlit: creating a browser app\n",
    "[Streamlit](https://docs.streamlit.io/) is a Python framework to create browser applications with little code. We will create a chatbot app through it.\n",
    "\n",
    "Run the following to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1c3cc-78ae-4593-8e34-379bb8f3ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU streamlit langgraph langchain-core langchain-postgres \"langchain[openai]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7f41d41-7424-4715-ba4d-9a12e2e151a7",
   "metadata": {},
   "source": [
    "Add the models' names and service URLs to the environment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce370cae-440c-4bc4-b916-f23257d7f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./streamlit-add.env\", \"w\") as env_file:\n",
    "    env_file.write(f\"CHAT_MODEL_NAME={chat_model_name}\\n\")\n",
    "    env_file.write(f\"CHAT_SERVICE_URL={chat_service_url}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_MODEL_NAME={embedding_model_name}\\n\")\n",
    "    env_file.write(f\"EMBEDDING_SERVICE_URL={embedding_service_url}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11394a1c-42c0-49db-aea5-e72c64ad7826",
   "metadata": {},
   "source": [
    "Define the Streamlit app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece29d3-0fbe-41a3-a5d3-5bec707b04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'rag-streamlit-app.py'\n",
    "import os\n",
    "import bs4\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langgraph.graph import START, StateGraph\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Read environment variables\n",
    "env_path = Path('.') / 'rag-kubeai.env'\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "add_env_path = Path('.') / 'streamlit-add.env'\n",
    "load_dotenv(dotenv_path=add_env_path, override=True)\n",
    "\n",
    "chat_model_name = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "chat_service_url = os.environ[\"CHAT_SERVICE_URL\"]\n",
    "embedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "embedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\n",
    "\n",
    "# Embedding model\n",
    "class CEmbeddings(OpenAIEmbeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n",
    "        emb_arr = []\n",
    "        for doc in docs:\n",
    "            embs = client.embeddings.create(\n",
    "                input=doc,\n",
    "                model=embedding_model_name\n",
    "            )\n",
    "            emb_arr.append(embs.data[0].embedding)\n",
    "        return emb_arr\n",
    "\n",
    "custom_embeddings = CEmbeddings(api_key=\"ignored\")\n",
    "\n",
    "# Vector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=custom_embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=os.environ[\"PG_CONN_URL\"],\n",
    ")\n",
    "\n",
    "# Chat model\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ignore\"\n",
    "llm = init_chat_model(chat_model_name, model_provider=\"openai\", base_url=f\"{chat_service_url}/v1/\")\n",
    "\n",
    "# Define prompt and operations\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Define graph of operations\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Streamlit setup\n",
    "st.title(\"RAG App\")\n",
    "st.write(\"Welcome to the RAG (Retrieval-Augmented Generation) app.\")\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "qa = st.container()\n",
    "\n",
    "with st.form(\"rag_form\", clear_on_submit=True):\n",
    "    question = st.text_input(\"Question\", \"\")\n",
    "    submit = st.form_submit_button(\"Submit\")\n",
    "    \n",
    "if submit:\n",
    "    # Load and chunk contents\n",
    "    if question:\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        with qa.chat_message(\"user\"):\n",
    "            st.write(question)\n",
    "    \n",
    "        response = graph.invoke({\"question\": question})\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(response[\"answer\"])\n",
    "    else:\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(\"You didn't provide a question!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a6f38dd-06a4-4a62-b5b0-2a8d66b1ed7f",
   "metadata": {},
   "source": [
    "## Launch and test the Streamlit app\n",
    "This command launches the Streamlit app, based on the file written by the previous cell. To access the app, you will need to forward port 8501 in Coder. Try asking the app the following question:\n",
    "    \n",
    "```\n",
    "What is task decomposition in LLM?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07957860-df31-4505-aeac-9c064f131dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run rag-streamlit-app.py --browser.gatherUsageStats false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OltreAI)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
