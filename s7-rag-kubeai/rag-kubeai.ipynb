{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7a1652-e4b9-4e96-b3ce-0c52f199c884",
   "metadata": {},
   "source": [
    "## 1. Initialize project\n",
    "Create or use an existing project to scope the RAG scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915e6ea-2cc7-4ba7-aca7-8d88c84f20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "project = dh.get_or_create_project(\"rag-kubeai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d74fc-4c28-4538-9734-de640cd7a313",
   "metadata": {},
   "source": [
    "### 1.1. Prepare secrets\n",
    "Two secrets are needed:\n",
    "- `HF_TOKEN`: The HuggingFace token, to use protected HuggingFace models\n",
    "- `PG_CONN_URL`: full PGVector DB URL to connect to the vector store. The value may be obtained from the platform configuration or a new DB may be created from KRM with the necessary extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524cd03-7912-499b-a1eb-c4b0389c77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b29fb-3eeb-463b-b566-5723e782e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path('.') / 'rag-kubeai.env'\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "project.new_secret(\"HF_TOKEN\", secret_value=os.environ[\"HF_TOKEN\"])\n",
    "project.new_secret(\"PG_CONN_URL\", secret_value=os.environ[\"PG_CONN_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117f5d2-2fb2-4cc8-8671-9047e4499c99",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for text generation\n",
    "For text generation, we deploy the `meta-llama/meta-llama-3-8b-instruct` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d9a77-cdd9-4a7b-b898-d724dc0a6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_func = project.new_function(\"chat\",\n",
    "                                    kind=\"kubeai-text\",\n",
    "                                    model_name=\"chatmodel\",\n",
    "                                    features=[\"TextGeneration\"],\n",
    "                                    url=\"hf://meta-llama/meta-llama-3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c1c33-7c93-42e3-9510-45792610806e",
   "metadata": {},
   "source": [
    "We run the function with the following parameters:\n",
    "- ``profile``: Execution profile for the node selection and resource usage (depends on the platform). In this example, 1xa100 refers to 1 GPU of type A100.\n",
    "- ``max_length``: Length of the context window for the text generation.\n",
    "- ``secrets``: List of secrets to pass to LLM. Needed if HuggingFace token is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf04441-c1eb-4f21-b82a-18aad7f3cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_run = chat_func.run(action=\"serve\",\n",
    "                           profile=\"1xa100\",\n",
    "                           max_length=\"5000\",\n",
    "                           secrets=[\"HF_TOKEN\"],\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb60e2e-bfef-44a4-9bbf-772ff737a3f7",
   "metadata": {},
   "source": [
    "Obtain the name of the deployed model and URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cc14c-b6fa-4511-b789-0302333e3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_name = chat_run.refresh().status.to_dict()[\"openai\"][\"model\"]\n",
    "chat_service_url = chat_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdccbe5-dbba-43de-a676-dc5fd1d37435",
   "metadata": {},
   "source": [
    "## 2. Deploy the supporting LLM for embeddings\n",
    "\n",
    "Embedding models map discrete data, such as words, to numerical vectors, which are more convenient for analysis, yet can still represent relationships between objects. We deploy the `thenlper/gte-base` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d24e8-80da-4d90-874f-fcab9f479428",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_func = project.new_function(\"emb\",\n",
    "                                kind=\"kubeai-text\",\n",
    "                                model_name=\"embmodel\",\n",
    "                                features=[\"TextEmbedding\"],\n",
    "                                engine=\"VLLM\",\n",
    "                                url=\"hf://thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a7f9a-f6fa-4c7b-af40-9373c121e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_run = emb_func.run(action=\"serve\",\n",
    "                       wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361a49c-4e1a-48f1-bcb0-2b226b9f4a23",
   "metadata": {},
   "source": [
    "Obtain the name of the deployed model and URL of the deployed service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e3d7d-5587-4229-a2eb-a30571dec65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = emb_run.refresh().status.to_dict()[\"openai\"][\"model\"]\n",
    "embedding_service_url = emb_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d941f1-1127-474a-9fd9-6b122c9efbc9",
   "metadata": {},
   "source": [
    "## 3. Process the relevant information and store embeddings in the Vector storage\n",
    "\n",
    "In a RAG scenario, a typical task is to store the supporting information into the vector storage and use it later for the text generation. In our example, the relevant information is first scraped from a Web page URL and then stored into the platform using the provided PGVector storage. Two components are required:\n",
    "- Embeddings processor that uses Open Inference Protocol of our embedding model service:\n",
    "  ```python\n",
    "    hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "        api_key=\"ignore\",\n",
    "        api_url=f\"http://{os.environ[\"EMBEDDING_SERVICE_URL\"]}/v1/models/embmodel:predict\"\n",
    "    )\n",
    "    class CEmbeddings(HuggingFaceInferenceAPIEmbeddings):\n",
    "        def embed_documents(self, docs):\n",
    "            return hf_embeddings.embed_documents(docs)[\"predictions\"]\n",
    "\n",
    "    custom_embeddings = CEmbeddings(api_key=\"ignore\")\n",
    "  ```\n",
    "- PGVector storage from the platform:\n",
    "  ```python\n",
    "    vector_store = PGVector(\n",
    "        embeddings=custom_embeddings,\n",
    "        collection_name=\"my_docs\",\n",
    "        connection=os.environ[\"PG_CONN_URL\"],\n",
    "    )\n",
    "  ```\n",
    "\n",
    "We define a Python job to obtain the document, create chunks, and store their embeddings in the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8d321-b0ea-4498-a856-f0deb69777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageurl = \"https://lilianweng.github.io/posts/2023-06-23-agent/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57c785-89a2-4f1c-9a9c-dae76ebf4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_func = project.new_function(\"create-embeddings\", \n",
    "                                   kind=\"python\", \n",
    "                                   python_version=\"PYTHON3_10\",\n",
    "                                   code_src=\"src/embedding.py\",\n",
    "                                   handler=\"embed\",\n",
    "                                   requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08df848-4b26-4755-8e22-6628ad76cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = data_func.run(\n",
    "    action=\"job\", \n",
    "    parameters={\"url\": pageurl},\n",
    "    envs=[\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url},\n",
    "            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": embedding_model_name}\n",
    "        ],\n",
    "    secrets=[\"PG_CONN_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f19be-3739-46d4-a417-8dc15d97332f",
   "metadata": {},
   "source": [
    "The results of the elaboration are stored in the corresponding database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167f465-bf0c-4c83-9269-04d8a4b18fb6",
   "metadata": {},
   "source": [
    "## 4. Create RAG application API\n",
    "Once the components and data are in place, we can create a LangChain-based application and expose it as API in the platform. This will use the chat model service, the vector database, and the serverless functionality.\n",
    "\n",
    "We create and deploy the serverless function that interacts with the LLM and uses the vector store for retrieval. It uses a simple LangChaing graph composed out of two steps: retrieval and generation. The result of the generation is returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ad63-b4dd-4d6a-b524-7214fd975a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_func = project.new_function(\n",
    "    name=\"rag-service\", \n",
    "    kind=\"python\", \n",
    "    python_version=\"PYTHON3_10\", \n",
    "    code_src=\"src/serve.py\",     \n",
    "    handler=\"serve\",\n",
    "    init_function=\"init\",\n",
    "    requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e72dd-b51f-4b2b-8309-50da8901166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run = serve_func.run(\n",
    "    action=\"serve\",\n",
    "    envs=[\n",
    "            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": embedding_service_url},\n",
    "            {\"name\": \"CHAT_SERVICE_URL\", \"value\": chat_service_url},\n",
    "            {\"name\": \"CHAT_MODEL_NAME\", \"value\": chat_model_name},\n",
    "            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": embedding_model_name}\n",
    "         ],\n",
    "    secrets=[\"PG_CONN_URL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7869430-d804-47f9-b8d2-a52f554962c5",
   "metadata": {},
   "source": [
    "To test our API we make a call to the service endpoint providing a JSON with the example question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96ac1c-89dd-4588-9598-ed225e6818ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_run.refresh().status.to_dict()[\"service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4ea4a-46f2-4d1c-883c-d665e2d6ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "serve_service_url = serve_run.refresh().status.to_dict()[\"service\"][\"url\"]\n",
    "\n",
    "res = requests.post(f\"http://{serve_service_url}\",json={\"question\": \"What is decomposition in LLM?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742f1c9-d5b9-486d-9097-3a2258849e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OltreAI)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
