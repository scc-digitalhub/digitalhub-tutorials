{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b4ce7f-4f95-4c61-b2d4-c14fa177d6e7",
   "metadata": {},
   "source": [
    "## 1. Initialize project\n",
    "Create or use an existing project to scope the RAG scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882d4d7e-cbf4-49f7-81ef-be5c15fc93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raman/python3.10/lib/python3.10/site-packages/digitalhub/stores/client/dhcore/configurator.py:96: UserWarning: Backend API level is higher than library version. You should consider updating the library.\n",
      "  warn(\"Backend API level is higher than library version. You should consider updating the library.\")\n"
     ]
    }
   ],
   "source": [
    "import  digitalhub as dh\n",
    "\n",
    "project = dh.get_or_create_project(\"rag-kubeai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ae423-444c-4367-a805-47dd3ff83609",
   "metadata": {},
   "source": [
    "## 2. Define and Deploy Training Function\n",
    "Training function is a python job that performs LLM fine tuning from a predefined HuggingFace model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6c0828-63ef-4e71-a98a-22404c631cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = project.new_function(\n",
    "    name=\"train-llm\", \n",
    "    kind=\"python\", \n",
    "    python_version=\"PYTHON3_10\", \n",
    "    code_src=\"src/llama_sft_training.py\",  \n",
    "    handler=\"train_and_log_model\",\n",
    "    requirements=[\"pandas==2.1.4\", \"tqdm==4.66.5\", \"openai==1.8.0\", \"spacy==3.7.5\", \"torch==2.5.1\", \"llama-index==0.9.33\", \"huggingface-hub==0.27.1\", \"rank-bm25==0.2.2\", \"sentence-transformers==4.1.0\", \"ranx==0.3.20\", \"transformers==4.48.0\", \"openpyxl==3.1.5\", \"rouge-score==0.1.2\", \"FlagEmbedding==1.3.4\", \"wandb==0.19.11\", \"nltk==3.8.1\", \"trl==0.13\", \"bitsandbytes==0.45.5\", \"datasets==3.6.0\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263657cf-b9d7-420c-be26-acfa08844072",
   "metadata": {},
   "source": [
    "## 3. Perform Training\n",
    "Training parameters define the model and dataset to be used. Training run refers to HF_TOKEN secret and expects a volume to be created of an appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aaa2f3b-55cf-4c55-b0c7-f02aa2520476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = func.run(action=\"job\",\n",
    "                     profile=\"1xa100\",\n",
    "                     parameters={\n",
    "                         \"model_id\": \"meta-llama/Llama-3.1-8B\",\n",
    "                         \"model_name\": \"llmpa-tracked\",\n",
    "                         \"hf_dataset_name\": \"team-bay/data-science-qa\",\n",
    "                         \"train_data_path\": \"DataScienceDataset.csv\",\n",
    "                         \"dev_data_path\": \"DataScienceDataset.csv\",\n",
    "                     },\n",
    "                     secrets=[\"HF_TOKEN\"],\n",
    "                     volumes=[{\n",
    "                        \"volume_type\": \"persistent_volume_claim\",\n",
    "                        \"name\": \"volume-llmpa\",\n",
    "                        \"mount_path\": \"/app/local_data\",\n",
    "                        \"spec\": { \"size\": \"20Gi\" }}]\n",
    "\t\t\t\t\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OltreAI)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
