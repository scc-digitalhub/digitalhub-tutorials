{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15142949-761b-4ac9-92aa-9d5750bfcb5a",
   "metadata": {},
   "source": [
    "# RAG: Retrieval Augmented Generation\n",
    "We will build a RAG example using [LangChain](https://python.langchain.com). Then, we will use [Streamlit](https://docs.streamlit.io/) to make a browser app from it.\n",
    "The code in this tutorial is largely taken from an [official LangChain tutorial](https://python.langchain.com/docs/tutorials/rag/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266009f3-f2fb-4a78-96f5-b91c212d93e7",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ddb29-69e7-448d-9f4d-fddcfa0bba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers==4.50.3 psycopg_binary streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf220e-fe8b-4e8e-860c-466a6af56172",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain-text-splitters langchain-community langgraph langchain-core langchain-huggingface langchain_postgres \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6af9f9-4cf4-4644-bc8c-8bf3989c85e3",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a30e5-b3cc-41f9-97ef-fb5ff8d5a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "project = dh.get_or_create_project(\"rag-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c5177-fbda-4b20-9413-022c5c153a32",
   "metadata": {},
   "source": [
    "### Configure API keys and other environment variables\n",
    "Configure the following environment variables in a `rag.env` file:\n",
    "\n",
    "- `HF_TOKEN`: Your HuggingFace token\n",
    "- `LANGSMITH_TRACING`: `true`\n",
    "- `LANGSMITH_API_KEY`: Your LangChain API key\n",
    "- `OPENAI_API_KEY`: Your OpenAI API key\n",
    "- `PG_CONN_URL`: Connection URL to the Postgres database for PGVector, in the format: `postgresql+psycopg://user:password@host:port/database`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63b531-d10c-40a4-817b-37d6fa087463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path(\".\") / \"rag.env\"\n",
    "load_dotenv(dotenv_path=env_path, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a941e6-08dd-4c85-be90-c4244e9ff803",
   "metadata": {},
   "source": [
    "### Chat model\n",
    "A [chat model](https://python.langchain.com/docs/concepts/chat_models/) can interpret and generate natural-language text. We create a function to launch a pre-existing model on the platform and serve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d7249-e3ee-411f-9727-52925b2b93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_func = project.new_function(\n",
    "    \"chat\", kind=\"huggingfaceserve\", model_name=\"chatmodel\", path=\"huggingface://meta-llama/meta-llama-3-8b-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bb448-c4ee-494f-83c4-76829ac4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_run = chat_func.run(\n",
    "    action=\"serve\",\n",
    "    profile=\"1xa100\",\n",
    "    max_length=\"5000\",\n",
    "    envs=[{\"name\": \"HF_TOKEN\", \"value\": os.environ[\"HF_TOKEN\"]}],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af410f47-5439-4414-ad72-c6cf34708fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_service_url = chat_run.refresh().status.to_dict()[\"service\"][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a85ffd-8006-4e84-bb7d-8ed8f2fdbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"chatmodel\", model_provider=\"openai\", base_url=f\"http://{chat_service_url}/openai/v1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd39f9-e1f5-4077-95df-3057cdba6274",
   "metadata": {},
   "source": [
    "### Embedding model\n",
    "[Embedding models](https://python.langchain.com/docs/concepts/embedding_models/) map discrete data, such as words, to numerical vectors, which are more convenient for analysis, yet can still represent relationships between objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6b9d5-a015-4fe9-9f05-0bd45c2aa63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_func = project.new_function(\n",
    "    \"emb\", kind=\"huggingfaceserve\", model_name=\"embmodel\", path=\"huggingface://thenlper/gte-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ded83e-c8aa-41ff-902e-ae504be7e029",
   "metadata": {},
   "source": [
    "At the moment, you must serve the model manually from the platform. Set the backend to `HUGGINGFACE` and the inference task to `text-embedding`. Once served, copy its service URL and assign it to the following variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b8274-daa1-4bcd-b7ac-d7f68542c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_service_url = \"<your_embedding_service_url>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205be04-8f51-4c86-9342-d84d35b85ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ[\"HF_TOKEN\"], api_url=f\"http://{embedding_service_url}/v1/models/embmodel:predict\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d15792-4c9e-4f4e-82ba-4d208f31ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEmbeddings(HuggingFaceInferenceAPIEmbeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        return hf_embeddings.embed_documents(docs)[\"predictions\"]\n",
    "\n",
    "\n",
    "custom_embeddings = CEmbeddings(api_key=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e22db7-e9f1-4b17-a1e4-08078a4dfde7",
   "metadata": {},
   "source": [
    "### Vector store\n",
    "Embeddings are stored in [vector stores](https://python.langchain.com/docs/concepts/vectorstores/), which allow for similarity searches, based on the semantic vicinity of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201000a-7b35-4f16-9327-cf0b6a18d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=custom_embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=os.environ[\"PG_CONN_URL\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e46d7f-f4cd-4787-a011-76c791be982b",
   "metadata": {},
   "source": [
    "### Load and chunk contents\n",
    "Split documents on new-line characters, as models have an easier time understanding the context of smaller inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d151e9-1b0e-4fdd-88f3-4d4f57edb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371898d7-a8e5-4959-b39d-4995ad18c622",
   "metadata": {},
   "source": [
    "### Define prompt and operations\n",
    "We define an object type to contain question, relevant context, and answer.\n",
    "\n",
    "We define two operations that will enrich this object: one takes the question and performs a similarity search to obtain and add context, the other uses the question and context to generate and add the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2509a99-e7c9-49fd-9888-f71bec611643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075b49f-b923-4cc8-8a84-50185cb45790",
   "metadata": {},
   "source": [
    "### Compile application\n",
    "We define a simple graph of operations: *retrieve* -> *generate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db36067-8657-4e87-9ad3-541240ce6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c73a3-9cfb-4861-b193-61e1efd87aa3",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a971-d38d-4458-86fe-bb67f9b4ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a73f1-2e10-4887-ab0e-4a5469296574",
   "metadata": {},
   "source": [
    "## Streamlit: creating a browser app\n",
    "[Streamlit](https://docs.streamlit.io/) is a Python framework to create browser applications with little code. We will create an app with a form, where the user will provide a document for retrieval and a question.\n",
    "\n",
    "Chat and embedding models must be available beforehand. If you've run the cells above, the models should be up, so we can environment variables to the services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917cc98-0a73-4ec5-b656-977cd329bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CHAT_SERVICE_URL\"] = chat_service_url\n",
    "os.environ[\"EMBEDDING_SERVICE_URL\"] = embedding_service_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23199f37-9bae-4304-b0ed-c0be79e24216",
   "metadata": {},
   "source": [
    "The following will create the Python file run by Streamlit. The code is largely the same as the steps above, with the necessary changes to define the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2017aa8-36b6-4e5c-b430-d5a829c988ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 'rag-streamlit-app.py'\n",
    "import os\n",
    "import bs4\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from pathlib import Path\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# API keys\n",
    "env_path = Path('.') / 'rag.env'\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "# Chat model\n",
    "llm = init_chat_model(\"chatmodel\", model_provider=\"openai\", base_url=f\"http://{os.environ['CHAT_SERVICE_URL']}/openai/v1/\")\n",
    "\n",
    "# Embedding model\n",
    "hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    "    api_url=f\"http://{os.environ['EMBEDDING_SERVICE_URL']}/v1/models/embmodel:predict\"\n",
    ")\n",
    "\n",
    "class CEmbeddings(HuggingFaceInferenceAPIEmbeddings):\n",
    "    def embed_documents(self, docs):\n",
    "        return hf_embeddings.embed_documents(docs)[\"predictions\"]\n",
    "custom_embeddings = CEmbeddings(api_key=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# Vector store\n",
    "vector_store = PGVector(\n",
    "    embeddings=custom_embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=os.environ[\"PG_CONN_URL\"],\n",
    ")\n",
    "\n",
    "# Define prompt and operations\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Define graph of operations\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Streamlit setup\n",
    "st.title(\"RAG App\")\n",
    "st.write(\"Welcome to the RAG (Retrieval-Augmented Generation) app.\")\n",
    "st.write(\"Please provide a link to the document to retrieve and your question.\")\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "qa = st.container()\n",
    "\n",
    "with st.form(\"rag_form\", clear_on_submit=True):\n",
    "    rag_document = st.text_input(\"Document\", \"\")\n",
    "    question = st.text_input(\"Question\", \"\")\n",
    "    submit = st.form_submit_button(\"Submit\")\n",
    "    \n",
    "if submit:\n",
    "    # Load and chunk contents\n",
    "    if question:\n",
    "        if rag_document:\n",
    "            loader = WebBaseLoader(\n",
    "                web_paths=(rag_document,),\n",
    "                bs_kwargs=dict(),\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            \n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            all_splits = text_splitter.split_documents(docs)\n",
    "            \n",
    "            # Index chunks\n",
    "            _ = vector_store.add_documents(documents=all_splits)\n",
    "    \n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        with qa.chat_message(\"user\"):\n",
    "            st.write(question)\n",
    "    \n",
    "        response = graph.invoke({\"question\": question})\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(response[\"answer\"])\n",
    "    else:\n",
    "        with qa.chat_message(\"assistant\"):\n",
    "            st.write(\"You didn't provide a question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ea142-39a5-4738-9ef1-fe3bfe692291",
   "metadata": {},
   "source": [
    "## Launch and test the Streamlit app\n",
    "This command launches the Streamlit app, based on the file written by the previous cell. To access the app, you will need to forward port 8501 in Coder. Try using the following to ask the app a question.\n",
    "\n",
    "Document:\n",
    "```\n",
    "https://lilianweng.github.io/posts/2023-06-23-agent\n",
    "```\n",
    "Question:\n",
    "```\n",
    "What is task decomposition?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87783b65-fc93-4fb4-86fa-9fe8d4be8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run rag-streamlit-app.py --browser.gatherUsageStats false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
