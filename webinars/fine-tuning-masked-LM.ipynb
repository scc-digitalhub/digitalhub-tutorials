{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65166213-22f3-488e-88ca-3e5cb5fb5928",
   "metadata": {},
   "source": [
    "# Deploying a Masked Language Model\n",
    "\n",
    "The DistilBERT language model derives from Google BERT language model, which can be used for masked language modeling and next sentence prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce223b-6a17-4044-9a57-29fd7e88c3b8",
   "metadata": {},
   "source": [
    "### 1. Define a project and a `huggingfaceserve` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f6e162-885b-4f80-8d07-d70f40634d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "project = dh.get_or_create_project(\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1c65ca-e2b8-4756-8bca-de30bd65b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function = project.new_function(\n",
    "    \"llm_masked\",\n",
    "    kind=\"huggingfaceserve\",\n",
    "    model_name=\"mymodel\",\n",
    "    path=\"huggingface://distilbert/distilbert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75899154-071b-4a44-b993-6d21a64d9a05",
   "metadata": {},
   "source": [
    "### 2. Serve the model\n",
    "\n",
    "LLM models have particular hardware requirements. When serving a model within the platform, you can use one of the preconfigured profiles, which define the resources that will be allocated.\n",
    "\n",
    "**NOTE**: when requesting a GPU node for the service, it may take some time for the service to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69100dac-917c-46b6-85ca-962389216d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed61aae-0e59-4e1e-9942-33359352eb7c",
   "metadata": {},
   "source": [
    "### 3. Try the model\n",
    "\n",
    "Note that BERT answers reflect its training on English Wikipedia and BookCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7f78840-4f77-41a3-b447-81f1250c59ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'mymodel',\n",
       " 'model_version': None,\n",
       " 'id': 'fde416ef-1b04-455e-8f2d-780c2a68b8af',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output-0',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['nocturnal']}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mymodel\"\n",
    "json = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input-0\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Cats are [MASK].\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "llm_run.invoke(model_name=model_name, json=json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6158bc-983b-404b-a198-ba8a9a62fcb7",
   "metadata": {},
   "source": [
    "# Adapt the Model on Movie Reviews Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ac4e7-5970-4544-8ed1-8095ffbb4206",
   "metadata": {},
   "source": [
    "### 1. Fine-tune the model\n",
    "\n",
    "Define and run a training function that will create a new model trained on the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373077d9-2c3d-4fdf-854a-13bb71fb2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"train_model.py\"\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from digitalhub_runtime_python import handler\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "@handler()\n",
    "def train(project):\n",
    "    model_id = \"distilbert/distilbert-base-uncased\"\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        result = tokenizer(examples[\"text\"])\n",
    "        if tokenizer.is_fast:\n",
    "            result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        return result\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    "    )\n",
    "\n",
    "    chunk_size = 128\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // chunk_size) * chunk_size\n",
    "        result = {\n",
    "            k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "    train_size = 10_000\n",
    "    test_size = int(0.1 * train_size)\n",
    "    \n",
    "    downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "        train_size=train_size, test_size=test_size, seed=42\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_id}-finetuned-imdb\",\n",
    "        overwrite_output_dir=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=downsampled_dataset[\"train\"],\n",
    "        eval_dataset=downsampled_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    save_dir = \"model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    trainer.save_model(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    project.log_model(\n",
    "        name=\"test_llm_model\",\n",
    "        kind=\"huggingface\",\n",
    "        base_model=model_id,\n",
    "        source=save_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73d6f48-c62e-447d-838a-1bbb1feddd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_func = project.new_function(\n",
    "    name=\"train_model\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"train_model.py\",\n",
    "    handler=\"train\",\n",
    "    requirements=[\n",
    "        \"hf_xet\",\n",
    "        \"datasets\",\n",
    "        \"transformers[torch]\",\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"accelerate\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332b502-3fab-4556-9c9a-2c18222df728",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = train_func.run(action=\"job\", profile=\"1xa100\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff6888-c6c7-4280-86b8-20eebced0efb",
   "metadata": {},
   "source": [
    "### 2. Serve the fine-tuned model\n",
    "\n",
    "Create and run the serving function (this will create a new version of the function created during the first step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb7d6ed-214f-4544-a3e1-b0b9b592ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dh.get_model(\"test_llm_model\", project=\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1344aa-c7ff-4642-b9d5-cc3afabcefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_function = project.new_function(\n",
    "    \"llm_masked\",\n",
    "    kind=\"huggingfaceserve\",\n",
    "    model_name=\"test_llm_model\",\n",
    "    path=model.spec.path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfcfcc-ae28-4d34-bb53-79c8b1fdc7bf",
   "metadata": {},
   "source": [
    "**NOTE**: at the time of writing, specifying a volume was a temporary workaround to overcome directory space limitations and might not be necessary anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496afaa9-713f-407c-a811-fdb99d764e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_run_finetuned = llm_function.run(\n",
    "    action=\"serve\",\n",
    "    profile=\"1xa100\",\n",
    "    volumes=[\n",
    "        {\n",
    "            \"name\": \"volumellm\",\n",
    "            \"volume_type\": \"empty_dir\",\n",
    "            \"mount_path\": \"/shared\",\n",
    "            \"spec\": {\"sizeLimit\": \"10Gi\"},\n",
    "        }\n",
    "    ],\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc79f0e-134e-4562-8af6-804de11fc1f5",
   "metadata": {},
   "source": [
    "### 3. Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dabd4f5-63ae-42a2-bcf1-6be2dfcf7c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'test_llm_model',\n",
       " 'model_version': None,\n",
       " 'id': '53893f51-a958-4101-bd66-86e7aadd0147',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output-0',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['movie']}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_finetuned = \"test_llm_model\"\n",
    "json = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input-0\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"This [MASK] was great.\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "llm_run_finetuned.invoke(model_name=model_name_finetuned, json=json).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450c7ae-c48f-40e2-8233-952716769d01",
   "metadata": {},
   "source": [
    "### 4. Create a Streamlit app\n",
    "\n",
    "Write the model name and the run key in an environment file that will be accessible by the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aafe390-5085-4fbc-b9f5-3e611ac7ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"model_name={model_name_finetuned}\\n\")\n",
    "    f.write(f\"model_run_key={llm_run_finetuned.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd7535-de0b-4eac-a486-1cb734d622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e84db9-ecf0-44eb-bd79-9f5092985fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"app.py\"\n",
    "import digitalhub as dh\n",
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "st.title(\"Chat Demo\")\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "if \"model_config\" not in st.session_state:\n",
    "    st.session_state[\"model_config\"] = {\n",
    "        \"model_name\": os.environ[\"model_name\"],\n",
    "        \"model_run\": dh.get_run(os.environ[\"model_run_key\"])\n",
    "    }\n",
    "\n",
    "#initialize a list to store chat history in the session state between reruns\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "#display messages (box with avatar and some content) from chat history on reruns\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "def get_completion(prompt):\n",
    "    input = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input-0\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [prompt]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    model_name = st.session_state.model_config[\"model_name\"]\n",
    "    run = st.session_state.model_config[\"model_run\"]\n",
    "\n",
    "    data_string = \"\"\n",
    "    for r in run.invoke(model_name=model_name, json=input):\n",
    "        data_string += r.decode('utf-8')\n",
    "\n",
    "    try:\n",
    "        return json.loads(data_string)[\"outputs\"][0][\"data\"][0]\n",
    "    except:\n",
    "        return \"Sorry, something went wrong.\"\n",
    "            \n",
    "#display a chat input and store its input in prompt\n",
    "if prompt := st.chat_input(\"Give me a sentence to fill (use [MASK] as placeholder)\"):\n",
    "    #store user input in history and display it as a chat message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    #display a chat message with the model response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response = get_completion(prompt)\n",
    "        st.markdown(response)\n",
    "    #store model response in history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e056504-32d4-4812-992a-faba16f7b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.3.101:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://172.213.107.20:8501\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94251f08-ed00-476b-b794-370cc16ab889",
   "metadata": {},
   "source": [
    "If you are running this notebook inside a Coder workspace, navigate to your workspace and click on \"Open ports\" to find a link to the Streamlit app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
